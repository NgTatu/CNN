{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from funtion_v2 import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(num_prevlay):\n",
    "\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "  \n",
    "    W1 = np.random.randn(3, 3, 3, 8)* 0.0001  #(f, f, n_C_prev)\n",
    "    b1 = np.random.randn(1, 1, 1, 8)* 0.0001\n",
    "    W2 = np.random.randn(1, num_prevlay)* 0.0001 \n",
    "    b2 = np.zeros((1, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (3, 3, 3, 8))\n",
    "    assert(b1.shape == (1, 1, 1, 8))\n",
    "    assert(W2.shape == (1, num_prevlay))\n",
    "    assert(b2.shape == ((1, 1)))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate, num_iterations, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a one-layer neural network: CONV -> RELU -> MAXPOOL -> SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, n_H, n_W, 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = ((X.shape[1]-8)/8+1)*((X.shape[1]-8)/8+1)*8                           # number of examples\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "  \n",
    "    parameters = initialize_parameters(m)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "\n",
    "        # Forward propagation: CONV -> RELU -> MAXPOOL -> SIGMOID. \n",
    "        \n",
    "        # CONV\n",
    "        Z1, cache_conv1 = conv_forward(X, W1, b1, hparameters={\"pad\" : 1, \"stride\" : 1})\n",
    "        ######\n",
    "        \n",
    "        #RELU\n",
    "        A1, cache_relu1 = relu(Z1)\n",
    "        cache_relu1= cache_relu1.reshape(A1.shape[0], -1).T\n",
    "        ######\n",
    "        \n",
    "        #MAXPOOL\n",
    "        P1, cache_maxpool = pool_forward(A1, hparameters={\"stride\" : 8 , \"f\" : 8 })\n",
    "        ######\n",
    "        \n",
    "        #Flatten P1\n",
    "        P1_flat = P1.reshape(P1.shape[0], -1).T\n",
    "        \n",
    "        #SIGMOID\n",
    "        A2, cache_sigmoid2 = linear_activation_forward(P1_flat, W2, b2, activation=\"sigmoid\")\n",
    "        ######\n",
    "        \n",
    "        # Compute cost\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation.\n",
    "        \n",
    "        #SIGMOID\n",
    "        dP1_flat, dW2, db2 = linear_activation_backward(dA2, cache_sigmoid2, activation=\"sigmoid\")\n",
    "        ########\n",
    "        \n",
    "        dP1 =  dP1_flat.reshape(P1.shape[0], P1.shape[1], P1.shape[2], P1.shape[3])\n",
    "        \n",
    "        #MAXPOOL\n",
    "        dA1 = pool_backward(dP1, cache_maxpool, mode = \"max\")\n",
    "        #######\n",
    "        \n",
    "        \n",
    "        dA1_flat = dA1.reshape(dA1.shape[0], -1).T\n",
    "        \n",
    "        \n",
    "        #RELU\n",
    "        dZ1_flat = relu_backward(dA1_flat, cache_relu1)\n",
    "        #######\n",
    "        \n",
    "        dZ1 = dZ1_flat.reshape(Z1.shape[0], Z1.shape[1], Z1.shape[2], Z1.shape[3])\n",
    "        \n",
    "        \n",
    "        #CONV\n",
    "        dA0, dW1, db1 = conv_backward(dZ1, cache_conv1)\n",
    "        #######\n",
    "    \n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 1 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
